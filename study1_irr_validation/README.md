# Study 1: Inter-Rater Reliability (IRR) Validation

This directory documents the Inter-Rater Reliability (IRR) analysis conducted to validate the perturbation coding scheme used in Study 1. The goal of this analysis was to ensure that the classification of "perturbations" is consistent and replicable.

## Overview

To establish the reliability of the coding, a subset of the data was coded independently by two raters. The agreement between these two sets of codes was then calculated to ensure the coding scheme is robust.

This folder contains the following documents:

-   **`methodology.md`**: A detailed description of the IRR protocol, including the definition of the coders, the coding process, and the statistical measures used.
-   **`results.md`**: A summary of the final reliability scores, including Cohen's Kappa and overall agreement percentage.

**Note:** The raw data and scripts used for this validation are not included in this repository to protect participant confidentiality and to keep the focus on the replication of the main LSH method. The documents here provide a complete summary of the validation procedure and its successful outcome.
